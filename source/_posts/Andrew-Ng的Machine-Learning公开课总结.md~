title: Andrew Ng的Machine Learning公开课总结
tags:
  - Machine Learning
date: 2014-10-18 20:55:18
catagories:
---

Tom Mitchell(1998)指出机器学习最重要的三个概念：Task（概括性的目标……），Performance(衡量指标)，Experience(训练过程)
这门课上提到的算法都是静态的。只着眼于一次采样。要通过一次训练尽可能好地拟合真实函数。
训练的过程，一般是通过最优化***cost function***来实现。
为简洁起见以下不说明任何符号含义，最后才附上说明。

##有监督学习
有监督学习可以做分类和估值。

**线性回归**
估值用，线性回归已经假设真实函数(目标函数)是线性的。
$h\_\theta(\mathbf{x}\_i) = \mathbf{\theta}^T\mathbf{x}\_i$ ， $\mathbf{h\_\theta(X)} = \mathbf{X\theta}$
其中用到的cost function，第二项是为了避免Overfitting:
$J(\mathbf{\theta}) = \frac{1}{2m}(h\_\theta(\mathbf{X})-\mathbf{y})^.2 + \lambda(\mathbf{\theta}\_{-0})^.2$
其中对于规模比较大的数据，需要多次***gradient descent***求解:
$\mathbf{\theta'} = \mathbf{\theta}-\alpha\frac{\partial}{\partial\mathbf{\theta}}J(\mathbf{\theta}) = \mathbf{\theta}-\alpha\frac{1}{m}(\mathbf{X}(h\_\theta(\mathbf{X})-\mathbf{y})+\lambda\mathbf{\theta}\_{-0})$
解同样可用数学方法***normal equation***获得，但计算逆矩阵要$O(n^3)$，计算量大，适合数据量小的情况，或者做验证用。
注意还要保证特征之间独立才会有逆矩阵，否则只能用伪逆代替:
$\mathbf{\theta} = (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{1}\_{-0})^{-1}\mathbf{X}^T\mathbf{y}$

**逻辑回归**
分类用，给回归函数套一个函数，保证值域是[0,1] : 
$h\_\theta(\mathbf{x}\_i) = sigmoid(\mathbf{\theta}^T\mathbf{x}\_i)$，其中$sigmoid(z) = \frac{1}{1+e^{-z}}$，斜s型的一个函数。
其中相应地cost function也要作出改变，才能是单峰的凸函数：
$J(\theta) = -\frac{1}{m}[\mathbf{y}^Tlogh\_\theta(\mathbf{X})+(1-\mathbf{y})^Tlog(1-h\_\theta(\mathbf{X}))] + \frac{\lambda}{2m}(\mathbf{\theta}\_{-0})^.2$
对于分多类的情况，每次训练一类，得到一个$h\_{\theta\_i}(\mathbf{X})$，然后求$argmax\\{i,h\_{\theta\_i}(\mathbf{X})\\}$就好了。

**参数调整**
线性回归中，如果$\alpha$太大，会overfitting，现象就是cost越来越大程序无法停止。太小又会跑得慢。
因此使用***Grid Search***，0.01, 0.03, 0.1, 0.3这样下去，直到回归速度足够快而又不过头。

##处理方法
各个特征值域不同，要做Feature Scaling与Mean Normalization:
$\mathbf{x'} = \frac{\mathbf{x}-\nu}{\mathbf{x\_{max}}-\mathbf{x\_{min}}}$
Overfitting: 去除冗余特征，再调整参数

----------------
符号说明：
点号表示逐元素运算，$\mathbf{x}^.2 = \\{x\_1^2,\ x\_2^2,\ \dots,\ x\_N^2\\}$
$X$输入矩阵，每行是一个输入矢量，size=(N\*M)，$y$采样值，size=(M\*1)，$\nu$均值

----------------

